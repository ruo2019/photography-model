{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Installing Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BpdJkdBssk9",
    "outputId": "3d9059a1-7707-4456-8d53-ba36e3d5bbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (6.2.3)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-jo9hl00j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-jo9hl00j\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (6.2.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "0caecc2b-0811-4663-8eb7-ba1af247121a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1\n",
      "CLIP Models: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import packaging\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display\n",
    "from os import system\n",
    "from PIL import Image, ImageTk\n",
    "import urllib.request\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import clip\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CLIP Models:\",clip.available_models())\n",
    "\n",
    "def pickle_read(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def pickle_write(a, b):\n",
    "    pickle_filename = a if len(a) >= 4 and a[-4:] == \".pkl\" else b\n",
    "    data = b if pickle_filename == a else a\n",
    "    with open(pickle_filename, 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "7d4e06e3-4d42-4e35-b1a6-bd390d4c0ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## Building features\n",
    "\n",
    "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MfirHzM2hvDe"
   },
   "outputs": [],
   "source": [
    "def loadImage(url, rows=5, cols=5):\n",
    "  original_images = []\n",
    "  urllib.request.urlretrieve(\n",
    "    url,\n",
    "    \"img.gif\")\n",
    "  im = Image.open(\"img.gif\")\n",
    "  try:\n",
    "    while 1:\n",
    "      im.seek(im.tell()+1)\n",
    "      original_images.append(im.convert(\"RGB\"))\n",
    "  except EOFError:\n",
    "    pass\n",
    "\n",
    "  print(len(original_images))\n",
    "\n",
    "  processed_images = []\n",
    "\n",
    "  for image in original_images:\n",
    "    processed_images.append(preprocess(image))\n",
    "\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  start = 0\n",
    "  for i in range(rows*cols):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(original_images[start+i])\n",
    "    plt.axis('off')\n",
    "    #plt.title(str(start+i))\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "  return original_images, processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "url = 'https://media1.giphy.com/media/lqdJsUDvJnHBgM82HB/giphy.gif'\n",
    "texts = ['a whale jumping out of water']\n",
    "orig_imgs, proc_imgs = loadImage(url,7,10)\n",
    "findMatch(orig_imgs, proc_imgs, texts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_to_mp4(video_path, output_dir, window_size=5):\n",
    "    if output_dir in os.listdir('.'):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if video_path.endswith('.gif'):\n",
    "        gif = Image.open(video_path)\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "        total_frames = len(frames)\n",
    "\n",
    "        width, height = frames[0].size\n",
    "        duration = gif.info['duration']\n",
    "        fps = 1000 / duration\n",
    "    else:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        frames = []\n",
    "        success, frame = video.read()\n",
    "        while success:\n",
    "            frames.append(frame)\n",
    "            success, frame = video.read()\n",
    "        video.release()\n",
    "\n",
    "    for i in range(total_frames - window_size + 1):\n",
    "        output_path = os.path.join(output_dir, f'{i + 1}.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames[i:i + window_size]:\n",
    "            if isinstance(frame, Image.Image):\n",
    "                frame_rgb = frame.convert('RGB')\n",
    "                frame_array = np.array(frame_rgb)\n",
    "                frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)\n",
    "            else:\n",
    "                frame_bgr = frame\n",
    "            out.write(frame_bgr)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "def load_basketball(basketball_path, size):\n",
    "    basketball = Image.open(basketball_path)\n",
    "    basketball = basketball.resize((size, size), Image.LANCZOS)\n",
    "    return basketball\n",
    "\n",
    "def rotate_basketball(basketball):\n",
    "    random_angle = np.random.randint(0, 360)\n",
    "    return basketball.rotate(random_angle, expand=True)\n",
    "\n",
    "def add_basketball_to_frame(frame, basketball):\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    basketball_rotated = rotate_basketball(basketball)\n",
    "\n",
    "    frame_width, frame_height = frame_pil.size\n",
    "    basketball_width, basketball_height = basketball_rotated.size\n",
    "\n",
    "    max_x = frame_width - basketball_width\n",
    "    max_y = frame_height - basketball_height\n",
    "    rand_x = np.random.randint(0, max_x)\n",
    "    rand_y = np.random.randint(0, max_y)\n",
    "\n",
    "    frame_pil.paste(basketball_rotated, (rand_x, rand_y), basketball_rotated)\n",
    "    return cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def add_noise(input_video_path, output_video_path, basketball_path, basketball_size):\n",
    "    basketball = load_basketball(basketball_path, basketball_size)\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame_with_basketball = add_basketball_to_frame(frame, basketball)\n",
    "        out.write(frame_with_basketball)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting Files\n",
    "```Python\n",
    "if 'aug1' in os.listdir('.'):\n",
    "    shutil.rmtree('aug1')\n",
    "if 'aug2' in os.listdir('.'):\n",
    "    shutil.rmtree('aug2')\n",
    "\n",
    "def get_dim(file_path):\n",
    "    vid = cv2.VideoCapture(file_path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    return height, width\n",
    "\n",
    "backflip_files = list(filter(lambda x: x != 'backflip/.DS_Store',[os.path.join('backflip', x) for x in os.listdir('backflip')]))\n",
    "\n",
    "os.mkdir('aug1')\n",
    "os.mkdir('aug2')\n",
    "for backflip in tqdm(backflip_files):\n",
    "    height, width = get_dim(backflip)\n",
    "    add_noise(backflip, os.path.join('aug1', backflip.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "for aug1 in tqdm([os.path.join('aug1',x) for x in os.listdir('aug1')]):\n",
    "    height, width = get_dim(aug1)\n",
    "    add_noise(aug1, os.path.join('aug2', aug1.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "shutil.rmtree('aug1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_images = {}\n",
    "\n",
    "def loadGifLocal(local_path, rows=5, cols=5):\n",
    "    if local_path in cached_images: return cached_images[local_path]\n",
    "    original_images = []\n",
    "\n",
    "    im = Image.open(local_path)\n",
    "    \n",
    "    try:\n",
    "        im.seek(im.tell())\n",
    "        original_images.append(im.convert(\"RGB\"))\n",
    "        while 1:\n",
    "            im.seek(im.tell()+1)\n",
    "            original_images.append(im.convert(\"RGB\"))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    cached_images[local_path] = (original_images, processed_images)\n",
    "    return original_images, processed_images\n",
    "\n",
    "def loadMP4Local(local_path, rows=5, cols=5):\n",
    "    video = cv2.VideoCapture(local_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = video.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame))\n",
    "        success, frame = video.read()\n",
    "    video.release()\n",
    "\n",
    "    original_images = frames\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    return original_images, processed_images\n",
    "\n",
    "def findMatch(original_images, processed_images, texts):\n",
    "  t1 = time.perf_counter()\n",
    "  image_input = torch.tensor(np.stack(processed_images))\n",
    "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts])\n",
    "\n",
    "  with torch.no_grad():\n",
    "      image_features = model.encode_image(image_input).float()\n",
    "      text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "  similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "  t2 = time.perf_counter()\n",
    "  print('time (sec) taken to run:',t2-t1)\n",
    "\n",
    "  #print(similarity)\n",
    "  plt.figure(figsize=(18, 6*len(texts)))\n",
    "  y_pred = []\n",
    "  for i, text in enumerate(texts):\n",
    "    plt.subplot(len(texts),2,1+2*i)\n",
    "    plt.plot(range(len(similarity[i])), similarity[i])\n",
    "    y_pred.append(similarity[i])\n",
    "    plt.title(\"best match is: \"+ str(np.argmax(similarity[i])), fontdict={'fontsize': 40})\n",
    "    plt.subplot(len(texts),2,2+2*i)\n",
    "    plt.imshow(original_images[np.argmax(similarity[i])])\n",
    "    plt.title(text, fontdict={'fontsize': 40})\n",
    "  plt.tight_layout()\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFrames(path, highlight=False):\n",
    "    global current_pick\n",
    "    video_path = path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        frames.append(img)\n",
    "        success, frame = cap.read()\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No frames found in the video.\")\n",
    "        return\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(path)\n",
    "    currIdx = -1\n",
    "\n",
    "    def display_frame(index):\n",
    "        global currIdx\n",
    "        frame_label.config(text=f\"Frame {index}\")\n",
    "        img = ImageTk.PhotoImage(frames[index])\n",
    "        frame_canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "        frame_canvas.image = img\n",
    "        currIdx = index\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Frame 0\", font=('Hack', 14), fg=\"red\" if highlight else \"black\")\n",
    "    frame_label.pack()\n",
    "    \n",
    "    frame_canvas = tk.Canvas(root, width=frames[0].width, height=frames[0].height)\n",
    "    frame_canvas.pack()\n",
    "    \n",
    "    display_frame(0)\n",
    "    \n",
    "    # Add an Entry widget for user input (text box)\n",
    "    filename_entry = tk.Entry(root, font=('Hack', 12))\n",
    "    filename_entry.pack(pady=10)\n",
    "    \n",
    "    def next_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame + 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doubleSkip(event):\n",
    "        next_frame(event)\n",
    "        next_frame(event)\n",
    "    \n",
    "    def prev_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame - 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doublePrev(event):\n",
    "        prev_frame(event)\n",
    "        prev_frame(event)\n",
    "\n",
    "    def save_frame(event):\n",
    "        global currIdx\n",
    "        global video_path\n",
    "\n",
    "        phrase = filename_entry.get()  # Get the text from the entry box\n",
    "        new_anno = pickle_read('rustyjar/anno_new.pkl')\n",
    "        new_anno.append((path, (int(path.split('/')[-1].split('.')[0]),phrase, currIdx + 1)))\n",
    "        pickle_write('rustyjar/anno_new.pkl', new_anno)\n",
    "        print(f\"Added Annotation for {path}!\")\n",
    "    \n",
    "    root.bind('<Right>', next_frame)\n",
    "    root.bind('<Left>', prev_frame)\n",
    "    root.bind('<Up>', doubleSkip)\n",
    "    root.bind('<Down>', doublePrev)\n",
    "    root.bind('<Command-s>', save_frame)\n",
    "    \n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 15:40:22.085 python[36349:29193824] NSEventModifierFlagFunction specified to -setKeyEquivalentModifierMask: for item <NSMenuItem: 0x600009599f10 Check Battery Level, ke='Command-F19'>, but is only supported for system-provided menu items; will not be used\n",
      "2024-09-17 15:40:22.085 python[36349:29193824] NSEventModifierFlagFunction specified to -setKeyEquivalentModifierMask: for item <NSMenuItem: 0x60000959a3e0 Send Email, ke='Command-F10'>, but is only supported for system-provided menu items; will not be used\n"
     ]
    }
   ],
   "source": [
    "showFrames(\"GIF87/1.mp4\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in list(filter(lambda x: int(x.split('/')[-1].split('.')[0]) in update_gif_list,[os.path.join('GIF87', x) for x in os.listdir('GIF87')]))[3:]:\n",
    "    anno = pickle_read('rustyjar/anno_new.pkl')\n",
    "    if i in [x[0] for x in anno]:\n",
    "        continue\n",
    "    showFrames(i)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_imgs, proc_imgs = loadMP4Local('augment/37.mp4')\n",
    "result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bGFUKNPhoAv",
    "outputId": "44177d64-d701-453d-b1a7-02f295c0d3a5"
   },
   "source": [
    "### Running Prediction\n",
    "```Python\n",
    "clip_pred = []\n",
    "\n",
    "backflip_files = os.listdir('augment')\n",
    "backflip_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "backflip_files = [os.path.join('augment', x) for x in backflip_files]\n",
    "\n",
    "pbar = tqdm(backflip_files)\n",
    "for video in pbar:\n",
    "    pbar.set_description(video.split('/')[1])\n",
    "    orig_imgs, proc_imgs = loadMP4Local(video)\n",
    "    result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "    \n",
    "    clip_pred.append(result)\n",
    "    print(str(result), end = \" | \")\n",
    "\n",
    "with open('predictions (updated)/CLIP_pred_aug.pkl', 'wb') as file:\n",
    "    pickle.dump([x.item() for x in clip_pred], file)\n",
    "\n",
    "print(\"Done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from `photo_data.csv`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "photo_data_csv = pd.read_csv(\"photo_data.csv\")\n",
    "photo_data_csv.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labeled_data = photo_data_csv.iloc[:, :3]\n",
    "\n",
    "labeled_data = list(labeled_data.itertuples(index=False, name=None))\n",
    "\n",
    "labeled_data = list(filter(lambda x: not math.isnan(x[0]), labeled_data))\n",
    "labeled_data.sort(key = lambda x: x[0])\n",
    "\n",
    "labeled_data = list(map(lambda x: (f'GIF87/{int(x[0])}.mp4', x[1], int(x[2])), labeled_data))\n",
    "\n",
    "data = {}\n",
    "for video, phrase, annotation in labeled_data:\n",
    "    if video not in data:\n",
    "        data[video] = (int(video.split('/')[-1].split('.')[0]), phrase, annotation)\n",
    "data = list(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GIF87/1.mp4', (1, 'woman falls down', 19))\n",
      "('GIF87/2.mp4', (2, 'woman falls down', 9))\n",
      "('GIF87/3.mp4', (3, 'guy falls down onto couch', 28))\n",
      "('GIF87/4.mp4', (4, 'person falls down', 14))\n",
      "('GIF87/5.mp4', (5, 'guy falls down', 10))\n"
     ]
    }
   ],
   "source": [
    "data = pickle_read(\"rustyjar/GIF87.pkl\")\n",
    "result = [print(data[x]) for x in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "X_lengths = []\n",
    "\n",
    "X_frames = []\n",
    "\n",
    "for gif_path, phrase, frame in tqdm(labeled_data):\n",
    "    orig_imgs, proc_imgs = loadImageLocal(gif_path)\n",
    "    X_lengths.append(len(orig_imgs))\n",
    "    curr_frames = []\n",
    "    for f_index, frame_pic in enumerate(orig_imgs):\n",
    "        append = \"\"\n",
    "        if f_index < frame:\n",
    "            append = \" before\" * min(50, frame - f_index)\n",
    "        elif f_index > frame:\n",
    "            append = \" after\" * min(50, f_index - frame)\n",
    "        else:\n",
    "            append = \"\"\n",
    "        after = \"\"\n",
    "        if f_index == frame:\n",
    "            after = ' now'\n",
    "        curr_frames.append((frame_pic, append + phrase + after))\n",
    "    X_frames.append(curr_frames)\n",
    "\n",
    "X_lengths = np.array(X_lengths)\n",
    "\n",
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
