{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Installing Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BpdJkdBssk9",
    "outputId": "3d9059a1-7707-4456-8d53-ba36e3d5bbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (6.2.3)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-ek_01s7b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-ek_01s7b\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (6.2.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "0caecc2b-0811-4663-8eb7-ba1af247121a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1\n",
      "CLIP Models: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import packaging\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display\n",
    "from os import system\n",
    "from PIL import Image, ImageTk\n",
    "import urllib.request\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import clip\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CLIP Models:\",clip.available_models())\n",
    "\n",
    "def pickle_read(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def pickle_write(a, b):\n",
    "    pickle_filename = a if len(a) >= 4 and a[-4:] == \".pkl\" else b\n",
    "    data = b if pickle_filename == a else a\n",
    "    with open(pickle_filename, 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "7d4e06e3-4d42-4e35-b1a6-bd390d4c0ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## Building features\n",
    "\n",
    "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MfirHzM2hvDe"
   },
   "outputs": [],
   "source": [
    "def loadImage(url, rows=5, cols=5):\n",
    "  original_images = []\n",
    "  urllib.request.urlretrieve(\n",
    "    url,\n",
    "    \"img.gif\")\n",
    "  im = Image.open(\"img.gif\")\n",
    "  try:\n",
    "    while 1:\n",
    "      im.seek(im.tell()+1)\n",
    "      original_images.append(im.convert(\"RGB\"))\n",
    "  except EOFError:\n",
    "    pass\n",
    "\n",
    "  print(len(original_images))\n",
    "\n",
    "  processed_images = []\n",
    "\n",
    "  for image in original_images:\n",
    "    processed_images.append(preprocess(image))\n",
    "\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  start = 0\n",
    "  for i in range(rows*cols):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(original_images[start+i])\n",
    "    plt.axis('off')\n",
    "    #plt.title(str(start+i))\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "  return original_images, processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "url = 'https://media1.giphy.com/media/lqdJsUDvJnHBgM82HB/giphy.gif'\n",
    "texts = ['a whale jumping out of water']\n",
    "orig_imgs, proc_imgs = loadImage(url,7,10)\n",
    "findMatch(orig_imgs, proc_imgs, texts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_to_mp4(video_path, output_dir, window_size=5):\n",
    "    if output_dir in os.listdir('.'):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if video_path.endswith('.gif'):\n",
    "        gif = Image.open(video_path)\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "        total_frames = len(frames)\n",
    "\n",
    "        width, height = frames[0].size\n",
    "        duration = gif.info['duration']\n",
    "        fps = 1000 / duration\n",
    "    else:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        frames = []\n",
    "        success, frame = video.read()\n",
    "        while success:\n",
    "            frames.append(frame)\n",
    "            success, frame = video.read()\n",
    "        video.release()\n",
    "\n",
    "    for i in range(total_frames - window_size + 1):\n",
    "        output_path = os.path.join(output_dir, f'{i + 1}.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames[i:i + window_size]:\n",
    "            if isinstance(frame, Image.Image):\n",
    "                frame_rgb = frame.convert('RGB')\n",
    "                frame_array = np.array(frame_rgb)\n",
    "                frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)\n",
    "            else:\n",
    "                frame_bgr = frame\n",
    "            out.write(frame_bgr)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "def load_basketball(basketball_path, size):\n",
    "    basketball = Image.open(basketball_path)\n",
    "    basketball = basketball.resize((size, size), Image.LANCZOS)\n",
    "    return basketball\n",
    "\n",
    "def rotate_basketball(basketball):\n",
    "    random_angle = np.random.randint(0, 360)\n",
    "    return basketball.rotate(random_angle, expand=True)\n",
    "\n",
    "def add_basketball_to_frame(frame, basketball):\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    basketball_rotated = rotate_basketball(basketball)\n",
    "\n",
    "    frame_width, frame_height = frame_pil.size\n",
    "    basketball_width, basketball_height = basketball_rotated.size\n",
    "\n",
    "    max_x = frame_width - basketball_width\n",
    "    max_y = frame_height - basketball_height\n",
    "    rand_x = np.random.randint(0, max_x)\n",
    "    rand_y = np.random.randint(0, max_y)\n",
    "\n",
    "    frame_pil.paste(basketball_rotated, (rand_x, rand_y), basketball_rotated)\n",
    "    return cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def add_noise(input_video_path, output_video_path, basketball_path, basketball_size):\n",
    "    basketball = load_basketball(basketball_path, basketball_size)\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame_with_basketball = add_basketball_to_frame(frame, basketball)\n",
    "        out.write(frame_with_basketball)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting Files\n",
    "```Python\n",
    "if 'aug1' in os.listdir('.'):\n",
    "    shutil.rmtree('aug1')\n",
    "if 'aug2' in os.listdir('.'):\n",
    "    shutil.rmtree('aug2')\n",
    "\n",
    "def get_dim(file_path):\n",
    "    vid = cv2.VideoCapture(file_path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    return height, width\n",
    "\n",
    "backflip_files = list(filter(lambda x: x != 'backflip/.DS_Store',[os.path.join('backflip', x) for x in os.listdir('backflip')]))\n",
    "\n",
    "os.mkdir('aug1')\n",
    "os.mkdir('aug2')\n",
    "for backflip in tqdm(backflip_files):\n",
    "    height, width = get_dim(backflip)\n",
    "    add_noise(backflip, os.path.join('aug1', backflip.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "for aug1 in tqdm([os.path.join('aug1',x) for x in os.listdir('aug1')]):\n",
    "    height, width = get_dim(aug1)\n",
    "    add_noise(aug1, os.path.join('aug2', aug1.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "shutil.rmtree('aug1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_images = {}\n",
    "\n",
    "def loadGifLocal(local_path, rows=5, cols=5):\n",
    "    if local_path in cached_images: return cached_images[local_path]\n",
    "    original_images = []\n",
    "\n",
    "    im = Image.open(local_path)\n",
    "    \n",
    "    try:\n",
    "        im.seek(im.tell())\n",
    "        original_images.append(im.convert(\"RGB\"))\n",
    "        while 1:\n",
    "            im.seek(im.tell()+1)\n",
    "            original_images.append(im.convert(\"RGB\"))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    cached_images[local_path] = (original_images, processed_images)\n",
    "    return original_images, processed_images\n",
    "\n",
    "def loadMP4Local(local_path, rows=5, cols=5):\n",
    "    video = cv2.VideoCapture(local_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = video.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame))\n",
    "        success, frame = video.read()\n",
    "    video.release()\n",
    "\n",
    "    original_images = frames\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    return original_images, processed_images\n",
    "\n",
    "def findMatch(original_images, processed_images, texts):\n",
    "  t1 = time.perf_counter()\n",
    "  image_input = torch.tensor(np.stack(processed_images))\n",
    "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts])\n",
    "\n",
    "  with torch.no_grad():\n",
    "      image_features = model.encode_image(image_input).float()\n",
    "      text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "  similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "  t2 = time.perf_counter()\n",
    "  print('time (sec) taken to run:',t2-t1)\n",
    "\n",
    "  #print(similarity)\n",
    "  plt.figure(figsize=(18, 6*len(texts)))\n",
    "  y_pred = []\n",
    "  for i, text in enumerate(texts):\n",
    "    plt.subplot(len(texts),2,1+2*i)\n",
    "    plt.plot(range(len(similarity[i])), similarity[i])\n",
    "    y_pred.append(similarity[i])\n",
    "    plt.title(\"best match is: \"+ str(np.argmax(similarity[i])), fontdict={'fontsize': 40})\n",
    "    plt.subplot(len(texts),2,2+2*i)\n",
    "    plt.imshow(original_images[np.argmax(similarity[i])])\n",
    "    plt.title(text, fontdict={'fontsize': 40})\n",
    "  plt.tight_layout()\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFrames(path, highlight=False):\n",
    "    video_path = path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        frames.append(img)\n",
    "        success, frame = cap.read()\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No frames found in the video.\")\n",
    "        return\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(path)\n",
    "    currIdx = -1\n",
    "\n",
    "    def display_frame(index):\n",
    "        global currIdx\n",
    "        frame_label.config(text=f\"Frame {index}\")\n",
    "        img = ImageTk.PhotoImage(frames[index])\n",
    "        frame_canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "        frame_canvas.image = img\n",
    "        currIdx = index\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Frame 0\", font=('Hack', 14), fg=\"red\" if highlight else \"black\")\n",
    "    frame_label.pack()\n",
    "    \n",
    "    frame_canvas = tk.Canvas(root, width=frames[0].width, height=frames[0].height)\n",
    "    frame_canvas.pack()\n",
    "    \n",
    "    display_frame(0)\n",
    "    \n",
    "    # Add an Entry widget for user input (text box)\n",
    "    filename_entry = tk.Entry(root, font=('Hack', 12))\n",
    "    filename_entry.pack(pady=10)\n",
    "    filename_entry.insert(0, \"frame_\")  # Default filename prefix\n",
    "    \n",
    "    def next_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame + 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doubleSkip(event):\n",
    "        next_frame(event)\n",
    "        next_frame(event)\n",
    "    \n",
    "    def prev_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame - 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doublePrev(event):\n",
    "        prev_frame(event)\n",
    "        prev_frame(event)\n",
    "\n",
    "    def save_frame(event):\n",
    "        global currIdx\n",
    "        filename = filename_entry.get()  # Get the text from the entry box\n",
    "        if not filename:  # If the text box is empty, use a default name\n",
    "            filename = f'frame_{currIdx}.png'\n",
    "        else:\n",
    "            filename = f'{filename}.png'\n",
    "        frames[currIdx].save(filename)\n",
    "        print(f\"Saved as {filename}!\")\n",
    "    \n",
    "    root.bind('<Right>', next_frame)\n",
    "    root.bind('<Left>', prev_frame)\n",
    "    root.bind('<Up>', doubleSkip)\n",
    "    root.bind('<Down>', doublePrev)\n",
    "    root.bind('<Command-s>', save_frame)\n",
    "    \n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 12:39:32.839 python[21867:29044151] NSEventModifierFlagFunction specified to -setKeyEquivalentModifierMask: for item <NSMenuItem: 0x60000d127950 Check Battery Level, ke='Command-F19'>, but is only supported for system-provided menu items; will not be used\n",
      "2024-09-17 12:39:32.839 python[21867:29044151] NSEventModifierFlagFunction specified to -setKeyEquivalentModifierMask: for item <NSMenuItem: 0x60000d135a40 Send Email, ke='Command-F10'>, but is only supported for system-provided menu items; will not be used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as frame_sss.png!\n"
     ]
    }
   ],
   "source": [
    "showFrames(f'augment/39.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_imgs, proc_imgs = loadMP4Local('augment/37.mp4')\n",
    "result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bGFUKNPhoAv",
    "outputId": "44177d64-d701-453d-b1a7-02f295c0d3a5"
   },
   "source": [
    "### Running Prediction\n",
    "```Python\n",
    "clip_pred = []\n",
    "\n",
    "backflip_files = os.listdir('augment')\n",
    "backflip_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "backflip_files = [os.path.join('augment', x) for x in backflip_files]\n",
    "\n",
    "pbar = tqdm(backflip_files)\n",
    "for video in pbar:\n",
    "    pbar.set_description(video.split('/')[1])\n",
    "    orig_imgs, proc_imgs = loadMP4Local(video)\n",
    "    result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "    \n",
    "    clip_pred.append(result)\n",
    "    print(str(result), end = \" | \")\n",
    "\n",
    "with open('predictions (updated)/CLIP_pred_aug.pkl', 'wb') as file:\n",
    "    pickle.dump([x.item() for x in clip_pred], file)\n",
    "\n",
    "print(\"Done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from `photo_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Description</th>\n",
       "      <th>Correct Frame</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Gifs Completed</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>woman falls down</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>woman falls down</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>guy falls down onto couch</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>person falls down</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>guy falls down</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                Description  Correct Frame  Unnamed: 3  Unnamed: 4  \\\n",
       "0  1.0           woman falls down           19.0         NaN         NaN   \n",
       "1  2.0           woman falls down            9.0         NaN         NaN   \n",
       "2  3.0  guy falls down onto couch           28.0         NaN         NaN   \n",
       "3  4.0          person falls down           14.0         NaN         NaN   \n",
       "4  5.0             guy falls down           10.0         NaN         NaN   \n",
       "\n",
       "  Gifs Completed  Unnamed: 6 Unnamed: 7  Frequency  \n",
       "0              1         1.0       True          2  \n",
       "1              2         2.0       True          1  \n",
       "2              3         3.0       True          2  \n",
       "3              4         4.0       True          1  \n",
       "4              5         5.0       True          1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_data_csv = pd.read_csv(\"photo_data.csv\")\n",
    "photo_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = photo_data_csv.iloc[:, :3]\n",
    "\n",
    "labeled_data = list(labeled_data.itertuples(index=False, name=None))\n",
    "\n",
    "labeled_data = list(filter(lambda x: not math.isnan(x[0]), labeled_data))\n",
    "labeled_data.sort(key = lambda x: x[0])\n",
    "\n",
    "labeled_data = list(map(lambda x: (f'GIF87/{int(x[0])}.mp4', x[1], int(x[2])), labeled_data))\n",
    "\n",
    "data = {}\n",
    "for video, phrase, annotation in labeled_data:\n",
    "    if video not in data:\n",
    "        data[video] = (int(video.split('/')[-1].split('.')[0]), phrase, annotation)\n",
    "data = list(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_gif_list = pickle_read('rustyjar/updated-gifs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_write(newData, 'rustyjar/GIF87.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GIF87/1.mp4', (1, 'woman falls down', 19))\n",
      "('GIF87/2.mp4', (2, 'woman falls down', 9))\n",
      "('GIF87/3.mp4', (3, 'guy falls down onto couch', 28))\n",
      "('GIF87/4.mp4', (4, 'person falls down', 14))\n",
      "('GIF87/5.mp4', (5, 'guy falls down', 10))\n",
      "('GIF87/6.mp4', (6, 'guy falls onto ground', 29))\n",
      "('GIF87/7.mp4', (7, 'a small kid falls down', 22))\n",
      "('GIF87/8.mp4', (8, 'a dog catching a frisbee in mouth', 67))\n",
      "('GIF87/9.mp4', (9, 'a dog catching a frisbee in air', 19))\n",
      "('GIF87/10.mp4', (10, 'a dog catching a frisbee in midair', 40))\n",
      "('GIF87/13.mp4', (13, 'guy falls down', 10))\n",
      "('GIF87/18.mp4', (18, 'a dog catching a frisbee in air', 13))\n",
      "('GIF87/21.mp4', (21, 'fish eating fish', 8))\n",
      "('GIF87/23.mp4', (23, 'small kid falls on top of small kid', 51))\n",
      "('GIF87/24.mp4', (24, 'a dog catching a frisbee in air', 89))\n",
      "('GIF87/25.mp4', (25, 'a squirrel shaking its ears', 28))\n",
      "('GIF87/26.mp4', (26, 'dog jumping', 6))\n",
      "('GIF87/27.mp4', (27, 'fox jumping', 12))\n",
      "('GIF87/28.mp4', (28, 'cat flying', 26))\n",
      "('GIF87/29.mp4', (29, 'cat jumps on top of dog', 15))\n",
      "('GIF87/30.mp4', (30, 'a panda jumps onto another panda', 9))\n",
      "('GIF87/31.mp4', (31, 'goat jumping', 6))\n",
      "('GIF87/32.mp4', (32, 'cat jumps', 3))\n",
      "('GIF87/33.mp4', (33, 'a rabbit hops', 3))\n",
      "('GIF87/34.mp4', (34, 'a duck lands', 12))\n",
      "('GIF87/36.mp4', (36, 'a horse jumping over a hurdle', 16))\n",
      "('GIF87/37.mp4', (37, 'goat kicks black goat', 13))\n",
      "('GIF87/39.mp4', (39, 'a cat jumping', 16))\n",
      "('GIF87/40.mp4', (40, 'dog jumps through hoop', 19))\n",
      "('GIF87/41.mp4', (41, 'red ball hits kid', 61))\n",
      "('GIF87/42.mp4', (42, 'goat jumps', 14))\n",
      "('GIF87/44.mp4', (44, 'dog jumps through hoop', 19))\n",
      "('GIF87/45.mp4', (45, 'cat jumps', 12))\n",
      "('GIF87/47.mp4', (47, 'dog catches frisbee in mouth', 20))\n",
      "('GIF87/49.mp4', (49, 'guy jumps backward', 40))\n",
      "('GIF87/51.mp4', (51, 'horse jumps over hurdle', 15))\n",
      "('GIF87/52.mp4', (52, 'horse jumps over hurdle', 4))\n",
      "('GIF87/53.mp4', (53, 'horse jumps over hurdle', 18))\n",
      "('GIF87/54.mp4', (54, 'horse jumping over hurdle', 11))\n",
      "('GIF87/55.mp4', (55, 'a horse jumping over a hurdle', 6))\n",
      "('GIF87/57.mp4', (57, 'horse jumps over hurdle', 11))\n",
      "('GIF87/58.mp4', (58, 'guy landing in dirt', 27))\n",
      "('GIF87/60.mp4', (60, 'guy claps', 0))\n",
      "('GIF87/74.mp4', (74, 'basketball hits face', 32))\n",
      "('GIF87/83.mp4', (83, 'soccer ball hits guy in face', 21))\n",
      "('GIF87/86.mp4', (86, 'ball hits face', 4))\n",
      "('GIF87/87.mp4', (87, \"ball hits guy's head\", 2))\n"
     ]
    }
   ],
   "source": [
    "newData = []\n",
    "for gif, annotation in data:\n",
    "    if annotation[0] not in update_gif_list:\n",
    "        newData.append((gif, annotation))\n",
    "for j in newData:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "X_lengths = []\n",
    "\n",
    "X_frames = []\n",
    "\n",
    "for gif_path, phrase, frame in tqdm(labeled_data):\n",
    "    orig_imgs, proc_imgs = loadImageLocal(gif_path)\n",
    "    X_lengths.append(len(orig_imgs))\n",
    "    curr_frames = []\n",
    "    for f_index, frame_pic in enumerate(orig_imgs):\n",
    "        append = \"\"\n",
    "        if f_index < frame:\n",
    "            append = \" before\" * min(50, frame - f_index)\n",
    "        elif f_index > frame:\n",
    "            append = \" after\" * min(50, f_index - frame)\n",
    "        else:\n",
    "            append = \"\"\n",
    "        after = \"\"\n",
    "        if f_index == frame:\n",
    "            after = ' now'\n",
    "        curr_frames.append((frame_pic, append + phrase + after))\n",
    "    X_frames.append(curr_frames)\n",
    "\n",
    "X_lengths = np.array(X_lengths)\n",
    "\n",
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
