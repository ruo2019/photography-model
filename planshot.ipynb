{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "# Interacting with CLIP\n",
    "\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/qing/PycharmProjects/InternVideo/Data/InternVid\n"
     ]
    }
   ],
   "source": [
    "%cd ../InternVideo/Data/InternVid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Preparation for Colab\n",
    "\n",
    "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BpdJkdBssk9",
    "outputId": "3d9059a1-7707-4456-8d53-ba36e3d5bbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (6.2.3)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-mur_dlbz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-mur_dlbz\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (6.2.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "0caecc2b-0811-4663-8eb7-ba1af247121a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLFS29hnhlY4",
    "outputId": "942301df-6204-4839-8050-9efac9066934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "7d4e06e3-4d42-4e35-b1a6-bd390d4c0ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21slhZGCqANb"
   },
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
    "\n",
    "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6cpiIFHp9N6",
    "outputId": "dde02e3f-fa6d-4844-9043-c0d7ecbf7da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x175c2b8b0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwSB5jZki3Cj"
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGom156-i2kL",
    "outputId": "5cf8b35d-5984-42d7-cf7b-aa4651ae37f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   637,  1237,  2097, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize(\"one two three\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W8ARJVqBJXs"
   },
   "source": [
    "# Setting up input images and texts\n",
    "\n",
    "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
    "\n",
    "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tMc1AXzBlhzm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## Building features\n",
    "\n",
    "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MfirHzM2hvDe"
   },
   "outputs": [],
   "source": [
    "def loadImage(url, rows=5, cols=5):\n",
    "  original_images = []\n",
    "  urllib.request.urlretrieve(\n",
    "    url,\n",
    "    \"img.gif\")\n",
    "  im = Image.open(\"img.gif\")\n",
    "  try:\n",
    "    while 1:\n",
    "      im.seek(im.tell()+1)\n",
    "      original_images.append(im.convert(\"RGB\"))\n",
    "  except EOFError:\n",
    "    pass\n",
    "\n",
    "  print(len(original_images))\n",
    "\n",
    "  processed_images = []\n",
    "\n",
    "  for image in original_images:\n",
    "    processed_images.append(preprocess(image))\n",
    "\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  start = 0\n",
    "  for i in range(rows*cols):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(original_images[start+i])\n",
    "    plt.axis('off')\n",
    "    #plt.title(str(start+i))\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "  return original_images, processed_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_to_mp4(video_path, output_dir, window_size=5):\n",
    "    if output_dir in os.listdir('.'):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if video_path.endswith('.gif'):\n",
    "        gif = Image.open(video_path)\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "        total_frames = len(frames)\n",
    "\n",
    "        width, height = frames[0].size\n",
    "        duration = gif.info['duration']\n",
    "        fps = 1000 / duration\n",
    "    else:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        frames = []\n",
    "        success, frame = video.read()\n",
    "        while success:\n",
    "            frames.append(frame)\n",
    "            success, frame = video.read()\n",
    "        video.release()\n",
    "\n",
    "    for i in range(total_frames - window_size + 1):\n",
    "        output_path = os.path.join(output_dir, f'{i + 1}.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames[i:i + window_size]:\n",
    "            if isinstance(frame, Image.Image):\n",
    "                frame_rgb = frame.convert('RGB')\n",
    "                frame_array = np.array(frame_rgb)\n",
    "                frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)\n",
    "            else:\n",
    "                frame_bgr = frame\n",
    "            out.write(frame_bgr)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "def load_basketball(basketball_path, size):\n",
    "    basketball = Image.open(basketball_path)\n",
    "    basketball = basketball.resize((size, size), Image.LANCZOS)\n",
    "    return basketball\n",
    "\n",
    "def rotate_basketball(basketball):\n",
    "    random_angle = np.random.randint(0, 360)\n",
    "    return basketball.rotate(random_angle, expand=True)\n",
    "\n",
    "def add_basketball_to_frame(frame, basketball):\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    basketball_rotated = rotate_basketball(basketball)\n",
    "\n",
    "    frame_width, frame_height = frame_pil.size\n",
    "    basketball_width, basketball_height = basketball_rotated.size\n",
    "\n",
    "    max_x = frame_width - basketball_width\n",
    "    max_y = frame_height - basketball_height\n",
    "    rand_x = np.random.randint(0, max_x)\n",
    "    rand_y = np.random.randint(0, max_y)\n",
    "\n",
    "    frame_pil.paste(basketball_rotated, (rand_x, rand_y), basketball_rotated)\n",
    "    return cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def add_noise(input_video_path, output_video_path, basketball_path, basketball_size):\n",
    "    basketball = load_basketball(basketball_path, basketball_size)\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_with_basketball = add_basketball_to_frame(frame, basketball)\n",
    "\n",
    "        out.write(frame_with_basketball)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    #cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video_to_mp4('augment/6.mp4', output_dir='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_images = {}\n",
    "\n",
    "def loadGifLocal(local_path, rows=5, cols=5):\n",
    "    if local_path in cached_images: return cached_images[local_path]\n",
    "    original_images = []\n",
    "\n",
    "    im = Image.open(local_path)\n",
    "    \n",
    "    try:\n",
    "        im.seek(im.tell())\n",
    "        original_images.append(im.convert(\"RGB\"))\n",
    "        while 1:\n",
    "            im.seek(im.tell()+1)\n",
    "            original_images.append(im.convert(\"RGB\"))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    cached_images[local_path] = (original_images, processed_images)\n",
    "    return original_images, processed_images\n",
    "\n",
    "def loadMP4Local(local_path, rows=5, cols=5):\n",
    "    video = cv2.VideoCapture(local_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = video.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame))\n",
    "        success, frame = video.read()\n",
    "    video.release()\n",
    "\n",
    "    original_images = frames\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    return original_images, processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HBgCanxi8JKw"
   },
   "outputs": [],
   "source": [
    "def findMatch(original_images, processed_images, texts, show_output=False):\n",
    "    t1 = time.perf_counter()\n",
    "    image_input = torch.tensor(np.stack(processed_images))\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in texts])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input).float()\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    if show_output:\n",
    "        plt.figure(figsize=(18, 6 * len(texts)))\n",
    "    \n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        if show_output:\n",
    "            plt.subplot(len(texts), 2, 1 + 2 * i)\n",
    "            plt.plot(range(len(similarity[i])), similarity[i])\n",
    "        results.append(np.argmax(similarity[i]))\n",
    "        if show_output:\n",
    "            plt.subplot(len(texts), 2, 2 + 2 * i)\n",
    "            plt.imshow(original_images[np.argmax(similarity[i])])\n",
    "            plt.title(text, fontdict={'fontsize': 40})\n",
    "    if show_output:\n",
    "        plt.tight_layout()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BABY Use this code (ignore everything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "def showFrames(path):\n",
    "    video_path = path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        frames.append(img)\n",
    "        success, frame = cap.read()\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No frames found in the video.\")\n",
    "        return\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(\"Frame Selection\")\n",
    "    \n",
    "    def display_frame(index):\n",
    "        frame_label.config(text=f\"Frame {index}\")\n",
    "        img = ImageTk.PhotoImage(frames[index])\n",
    "        frame_canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "        frame_canvas.image = img\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Frame 0\", font=('Hack', 14))\n",
    "    frame_label.pack()\n",
    "    \n",
    "    frame_canvas = tk.Canvas(root, width=frames[0].width, height=frames[0].height)\n",
    "    frame_canvas.pack()\n",
    "    \n",
    "    display_frame(0)\n",
    "    \n",
    "    def next_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame + 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doubleSkip(event):\n",
    "        next_frame(event)\n",
    "        next_frame(event)\n",
    "    \n",
    "    def prev_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame - 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "    def doublePrev(event):\n",
    "        prev_frame(event)\n",
    "        prev_frame(event)\n",
    "    \n",
    "    root.bind('<Right>', next_frame)\n",
    "    root.bind('<Left>', prev_frame)\n",
    "    root.bind('r', doubleSkip)\n",
    "    root.bind('l', doublePrev)\n",
    "    \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "showFrames('194.mp4') # Show frames for a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "orig_imgs, proc_imgs = loadMP4Local('194.mp4')\n",
    "result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bGFUKNPhoAv",
    "outputId": "44177d64-d701-453d-b1a7-02f295c0d3a5"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "\n",
    "clip_pred = []\n",
    "\n",
    "backflip_files = os.listdir('augment')\n",
    "backflip_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "backflip_files = [os.path.join('augment', x) for x in backflip_files]\n",
    "\n",
    "pbar = tqdm(backflip_files)\n",
    "for video in pbar:\n",
    "    pbar.set_description(video.split('/')[1])\n",
    "    orig_imgs, proc_imgs = loadMP4Local(video)\n",
    "    result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "    \n",
    "    clip_pred.append(result)\n",
    "    print(str(result), end = \" | \")\n",
    "\n",
    "with open('aug_pkl/clip_pred.pkl', 'wb') as file:\n",
    "    pickle.dump([x.item() for x in clip_pred], file)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from `photo_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "photo_data_csv = pd.read_csv(\"photo_data.csv\")\n",
    "photo_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = photo_data_csv.iloc[:, :3]\n",
    "\n",
    "unfiltered_data_triples = list(all_labeled_data.itertuples(index=False, name=None))\n",
    "\n",
    "labeled_data_float = list(filter(lambda x: not math.isnan(x[0]), unfiltered_data_triples))\n",
    "\n",
    "labeled_data = list(map(lambda x: (f'gifs/{int(x[0])}.gif', x[1], int(x[2])), labeled_data_float))\n",
    "\n",
    "X = labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# y_pred = []\n",
    "X_lengths = []\n",
    "\n",
    "X_frames = []\n",
    "\n",
    "for gif_path, phrase, frame in tqdm(labeled_data):\n",
    "    orig_imgs, proc_imgs = loadImageLocal(gif_path)\n",
    "    X_lengths.append(len(orig_imgs))\n",
    "    curr_frames = []\n",
    "    for f_index, frame_pic in enumerate(orig_imgs):\n",
    "        append = \"\"\n",
    "        if f_index < frame:\n",
    "            append = \" before\" * min(50, frame - f_index)\n",
    "        elif f_index > frame:\n",
    "            append = \" after\" * min(50, f_index - frame)\n",
    "        else:\n",
    "            append = \"\"\n",
    "        after = \"\"\n",
    "        if f_index == frame:\n",
    "            after = ' now'\n",
    "        curr_frames.append((frame_pic, append + phrase + after))\n",
    "    X_frames.append(curr_frames)\n",
    "    \n",
    "    # prediction = findMatch(orig_imgs, proc_imgs, [phrase])[0]\n",
    "    # print(f\"Prediction for {gif_path}: Frame {prediction} vs. Actual: {frame}\")\n",
    "    # print(\"For the phrase '\" + phrase + \"'\")\n",
    "    # clear_output(wait=True)\n",
    "    # y_pred.append(prediction)\n",
    "\n",
    "# y_pred = np.array(y_pred)\n",
    "X_lengths = np.array(X_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:25]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:25]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_frames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('saved_model.pth', weights_only=True, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store results\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Loop through the test data\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "        images, texts = batch\n",
    "        print(images[0])\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Calculate accuracy or other metrics\n",
    "        # Assuming the task is image-text retrieval, where the correct text for an image is its corresponding index\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        image_predictions = logits_per_image.argmax(dim=1)\n",
    "        text_predictions = logits_per_text.argmax(dim=1)\n",
    "\n",
    "        print(image_predictions)\n",
    "        # print(text_predictions)\n",
    "        print(ground_truth)\n",
    "        correct_predictions += (image_predictions == ground_truth).sum().item()\n",
    "        total_samples += len(images)\n",
    "\n",
    "# Calculate final accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose computation device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image = list_image\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(self.image[idx])\n",
    "        title = self.title[idx]\n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cycle = 1  # Adjust this between 1 and 5\n",
    "\n",
    "slice_length = len(X_frames) // 5\n",
    "\n",
    "start_index = (train_cycle - 1) * slice_length\n",
    "end_index = start_index + slice_length\n",
    "\n",
    "X_test = X_frames[start_index:end_index]\n",
    "\n",
    "X_train = X_frames[:start_index] + X_frames[end_index:]\n",
    "\n",
    "print(f\"Training data for train_cycle {train_cycle}: {len(X_train)} elements\")\n",
    "print(f\"Testing data for train_cycle {train_cycle}: {len(X_test)} elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = []\n",
    "X_test_flatten = []\n",
    "for GIF in X_train:\n",
    "    for frame in GIF:\n",
    "        X_train_flatten.append(frame)\n",
    "for GIF in X_test:\n",
    "    for frame in GIF:\n",
    "        X_test_flatten.append(frame)\n",
    "\n",
    "X_train = X_train_flatten\n",
    "X_test = X_test_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromX(X):\n",
    "    list_image = []\n",
    "    list_txt = []\n",
    "    for image, desc in X:\n",
    "        img_path = image\n",
    "        caption = desc\n",
    "        list_image.append(img_path)\n",
    "        list_txt.append(caption)\n",
    "    return list_image, list_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image_train, list_txt_train = loadFromX(X_train)\n",
    "dataset_train = image_title_dataset(list_image_train, list_txt_train)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=100, shuffle=True) #Define your own dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image_test, list_txt_test = loadFromX(X_test)\n",
    "dataset_test = image_title_dataset(list_image_test, list_txt_test)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=100, shuffle=True) #Define your own dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()\n",
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch\n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else :\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade decorator==4.4.2\n",
    "!pip install ffmpeg --upgrade\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import sys\n",
    "\n",
    "def scale_video(input_file, output_file, target_duration):\n",
    "    # Load the video file\n",
    "    video = VideoFileClip(input_file)\n",
    "    \n",
    "    # Get the original duration of the video in seconds\n",
    "    original_duration = video.duration\n",
    "    \n",
    "    # Calculate the scaling factor\n",
    "    if original_duration > target_duration:\n",
    "        scale_factor = target_duration / original_duration\n",
    "    else:\n",
    "        scale_factor = 1  # No scaling needed if the video is shorter than the target duration\n",
    "\n",
    "    # Apply the speed-up effect to the video\n",
    "    scaled_video = video.fx(lambda clip: clip.speedx(factor=1/scale_factor))\n",
    "\n",
    "    new_audio = AudioFileClip(\"../../Desktop/tauchip.wav\")\n",
    "\n",
    "    # Set the new audio to the video clip\n",
    "    video_with_new_audio = scaled_video.set_audio(new_audio)\n",
    "    \n",
    "    # Write the result to an MP4 file\n",
    "    video_with_new_audio.write_videofile(\"output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "scale_video('../../Desktop/tauplayback.mov', '../../Desktop/tauscaled.mp4', 6*60 + 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load the video file and the WAV audio file\n",
    "video_clip = VideoFileClip(\"../../Desktop/tauscaled.mp4\")\n",
    "new_audio = AudioFileClip(\"../../Desktop/tauchip.wav\")\n",
    "\n",
    "# Set the new audio to the video clip\n",
    "video_with_new_audio = video_clip.set_audio(new_audio)\n",
    "\n",
    "# Write the result to an MP4 file\n",
    "video_with_new_audio.write_videofile(\"output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "print(\"Video and audio combined successfully into output.mp4!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
