{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Installing Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BpdJkdBssk9",
    "outputId": "3d9059a1-7707-4456-8d53-ba36e3d5bbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (6.2.3)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-mur_dlbz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/zx/46y7zv8x5gd6xfp0s16kr0w00000gp/T/pip-req-build-mur_dlbz\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (6.2.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/xclip/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "0caecc2b-0811-4663-8eb7-ba1af247121a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1\n",
      "CLIP Models: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import packaging\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display\n",
    "from os import system\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import skimage\n",
    "import pickle\n",
    "import torch\n",
    "import torch\n",
    "import time\n",
    "import clip\n",
    "import os\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CLIP Models:\",clip.available_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "7d4e06e3-4d42-4e35-b1a6-bd390d4c0ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## Building features\n",
    "\n",
    "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MfirHzM2hvDe"
   },
   "outputs": [],
   "source": [
    "def loadImage(url, rows=5, cols=5):\n",
    "  original_images = []\n",
    "  urllib.request.urlretrieve(\n",
    "    url,\n",
    "    \"img.gif\")\n",
    "  im = Image.open(\"img.gif\")\n",
    "  try:\n",
    "    while 1:\n",
    "      im.seek(im.tell()+1)\n",
    "      original_images.append(im.convert(\"RGB\"))\n",
    "  except EOFError:\n",
    "    pass\n",
    "\n",
    "  print(len(original_images))\n",
    "\n",
    "  processed_images = []\n",
    "\n",
    "  for image in original_images:\n",
    "    processed_images.append(preprocess(image))\n",
    "\n",
    "  plt.figure(figsize=(20, 10))\n",
    "  start = 0\n",
    "  for i in range(rows*cols):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(original_images[start+i])\n",
    "    plt.axis('off')\n",
    "    #plt.title(str(start+i))\n",
    "  plt.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "  return original_images, processed_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def split_video_to_mp4(video_path, output_dir, window_size=5):\n",
    "    if output_dir in os.listdir('.'):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if video_path.endswith('.gif'):\n",
    "        gif = Image.open(video_path)\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(gif)]\n",
    "        total_frames = len(frames)\n",
    "\n",
    "        width, height = frames[0].size\n",
    "        duration = gif.info['duration']\n",
    "        fps = 1000 / duration\n",
    "    else:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        frames = []\n",
    "        success, frame = video.read()\n",
    "        while success:\n",
    "            frames.append(frame)\n",
    "            success, frame = video.read()\n",
    "        video.release()\n",
    "\n",
    "    for i in range(total_frames - window_size + 1):\n",
    "        output_path = os.path.join(output_dir, f'{i + 1}.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames[i:i + window_size]:\n",
    "            if isinstance(frame, Image.Image):\n",
    "                frame_rgb = frame.convert('RGB')\n",
    "                frame_array = np.array(frame_rgb)\n",
    "                frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)\n",
    "            else:\n",
    "                frame_bgr = frame\n",
    "            out.write(frame_bgr)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "def load_basketball(basketball_path, size):\n",
    "    basketball = Image.open(basketball_path)\n",
    "    basketball = basketball.resize((size, size), Image.LANCZOS)\n",
    "    return basketball\n",
    "\n",
    "def rotate_basketball(basketball):\n",
    "    random_angle = np.random.randint(0, 360)\n",
    "    return basketball.rotate(random_angle, expand=True)\n",
    "\n",
    "def add_basketball_to_frame(frame, basketball):\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    basketball_rotated = rotate_basketball(basketball)\n",
    "\n",
    "    frame_width, frame_height = frame_pil.size\n",
    "    basketball_width, basketball_height = basketball_rotated.size\n",
    "\n",
    "    max_x = frame_width - basketball_width\n",
    "    max_y = frame_height - basketball_height\n",
    "    rand_x = np.random.randint(0, max_x)\n",
    "    rand_y = np.random.randint(0, max_y)\n",
    "\n",
    "    frame_pil.paste(basketball_rotated, (rand_x, rand_y), basketball_rotated)\n",
    "    return cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def add_noise(input_video_path, output_video_path, basketball_path, basketball_size):\n",
    "    basketball = load_basketball(basketball_path, basketball_size)\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame_with_basketball = add_basketball_to_frame(frame, basketball)\n",
    "        out.write(frame_with_basketball)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c848df003447ccbdc70f31aa4a3e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d055d0d40cf465a9dfd5c816f7de010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'aug1' in os.listdir('.'):\n",
    "    shutil.rmtree('aug1')\n",
    "if 'aug2' in os.listdir('.'):\n",
    "    shutil.rmtree('aug2')\n",
    "\n",
    "def get_dim(file_path):\n",
    "    vid = cv2.VideoCapture(file_path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    return height, width\n",
    "\n",
    "backflip_files = list(filter(lambda x: x != 'backflip/.DS_Store',[os.path.join('backflip', x) for x in os.listdir('backflip')]))\n",
    "\n",
    "os.mkdir('aug1')\n",
    "os.mkdir('aug2')\n",
    "for backflip in tqdm(backflip_files):\n",
    "    height, width = get_dim(backflip)\n",
    "    add_noise(backflip, os.path.join('aug1', backflip.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "for aug1 in tqdm([os.path.join('aug1',x) for x in os.listdir('aug1')]):\n",
    "    height, width = get_dim(aug1)\n",
    "    add_noise(aug1, os.path.join('aug2', aug1.split('/')[1]), '../../../Storage/cruise.png', int(min(height, width) / 3))\n",
    "\n",
    "shutil.rmtree('aug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video_to_mp4('augment/6.mp4', output_dir='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_images = {}\n",
    "\n",
    "def loadGifLocal(local_path, rows=5, cols=5):\n",
    "    if local_path in cached_images: return cached_images[local_path]\n",
    "    original_images = []\n",
    "\n",
    "    im = Image.open(local_path)\n",
    "    \n",
    "    try:\n",
    "        im.seek(im.tell())\n",
    "        original_images.append(im.convert(\"RGB\"))\n",
    "        while 1:\n",
    "            im.seek(im.tell()+1)\n",
    "            original_images.append(im.convert(\"RGB\"))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    cached_images[local_path] = (original_images, processed_images)\n",
    "    return original_images, processed_images\n",
    "\n",
    "def loadMP4Local(local_path, rows=5, cols=5):\n",
    "    video = cv2.VideoCapture(local_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = video.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame))\n",
    "        success, frame = video.read()\n",
    "    video.release()\n",
    "\n",
    "    original_images = frames\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for image in original_images:\n",
    "        processed_images.append(preprocess(image))\n",
    "\n",
    "    return original_images, processed_images\n",
    "\n",
    "def findMatch(original_images, processed_images, texts, show_output=False):\n",
    "    t1 = time.perf_counter()\n",
    "    image_input = torch.tensor(np.stack(processed_images))\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in texts])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input).float()\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    if show_output:\n",
    "        plt.figure(figsize=(18, 6 * len(texts)))\n",
    "    \n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        if show_output:\n",
    "            plt.subplot(len(texts), 2, 1 + 2 * i)\n",
    "            plt.plot(range(len(similarity[i])), similarity[i])\n",
    "        results.append(np.argmax(similarity[i]))\n",
    "        if show_output:\n",
    "            plt.subplot(len(texts), 2, 2 + 2 * i)\n",
    "            plt.imshow(original_images[np.argmax(similarity[i])])\n",
    "            plt.title(text, fontdict={'fontsize': 40})\n",
    "    if show_output:\n",
    "        plt.tight_layout()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BABY Use this code (ignore everything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "\n",
    "def showFrames(path, highlight = False):\n",
    "    video_path = path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        frames.append(img)\n",
    "        success, frame = cap.read()\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No frames found in the video.\")\n",
    "        return\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.title(path)\n",
    "    \n",
    "    def display_frame(index):\n",
    "        frame_label.config(text=f\"Frame {index}\")\n",
    "        img = ImageTk.PhotoImage(frames[index])\n",
    "        frame_canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
    "        frame_canvas.image = img\n",
    "    \n",
    "    frame_label = tk.Label(root, text=\"Frame 0\", font=('Hack', 14), fg=\"red\" if highlight else \"black\")\n",
    "    frame_label.pack()\n",
    "    \n",
    "    frame_canvas = tk.Canvas(root, width=frames[0].width, height=frames[0].height)\n",
    "    frame_canvas.pack()\n",
    "    \n",
    "    display_frame(0)\n",
    "    \n",
    "    def next_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame + 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "\n",
    "    def doubleSkip(event):\n",
    "        next_frame(event)\n",
    "        next_frame(event)\n",
    "    \n",
    "    def prev_frame(event):\n",
    "        current_frame = int(frame_label.cget(\"text\").split()[1])\n",
    "        next_index = (current_frame - 1) % len(frames)\n",
    "        display_frame(next_index)\n",
    "    def doublePrev(event):\n",
    "        prev_frame(event)\n",
    "        prev_frame(event)\n",
    "    \n",
    "    root.bind('<Right>', next_frame)\n",
    "    root.bind('<Left>', prev_frame)\n",
    "    root.bind('r', doubleSkip)\n",
    "    root.bind('l', doublePrev)\n",
    "    \n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 51): \n",
    "    showFrames(f'augment/{i}.mp4', highlight= (i in [2,3,5,24,30,38,40,46,48])) # Show frames for a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "orig_imgs, proc_imgs = loadMP4Local('194.mp4')\n",
    "result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bGFUKNPhoAv",
    "outputId": "44177d64-d701-453d-b1a7-02f295c0d3a5"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "\n",
    "clip_pred = []\n",
    "\n",
    "backflip_files = os.listdir('augment')\n",
    "backflip_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "backflip_files = [os.path.join('augment', x) for x in backflip_files]\n",
    "\n",
    "pbar = tqdm(backflip_files)\n",
    "for video in pbar:\n",
    "    pbar.set_description(video.split('/')[1])\n",
    "    orig_imgs, proc_imgs = loadMP4Local(video)\n",
    "    result = findMatch(orig_imgs, proc_imgs, ['A person performs a backflip.'])[0]\n",
    "    \n",
    "    clip_pred.append(result)\n",
    "    print(str(result), end = \" | \")\n",
    "\n",
    "with open('predictions/clip_pred.pkl', 'wb') as file:\n",
    "    pickle.dump([x.item() for x in clip_pred], file)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from `photo_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "photo_data_csv = pd.read_csv(\"photo_data.csv\")\n",
    "photo_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = photo_data_csv.iloc[:, :3]\n",
    "\n",
    "unfiltered_data_triples = list(all_labeled_data.itertuples(index=False, name=None))\n",
    "\n",
    "labeled_data_float = list(filter(lambda x: not math.isnan(x[0]), unfiltered_data_triples))\n",
    "\n",
    "labeled_data = list(map(lambda x: (f'gifs/{int(x[0])}.gif', x[1], int(x[2])), labeled_data_float))\n",
    "\n",
    "X = labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# y_pred = []\n",
    "X_lengths = []\n",
    "\n",
    "X_frames = []\n",
    "\n",
    "for gif_path, phrase, frame in tqdm(labeled_data):\n",
    "    orig_imgs, proc_imgs = loadImageLocal(gif_path)\n",
    "    X_lengths.append(len(orig_imgs))\n",
    "    curr_frames = []\n",
    "    for f_index, frame_pic in enumerate(orig_imgs):\n",
    "        append = \"\"\n",
    "        if f_index < frame:\n",
    "            append = \" before\" * min(50, frame - f_index)\n",
    "        elif f_index > frame:\n",
    "            append = \" after\" * min(50, f_index - frame)\n",
    "        else:\n",
    "            append = \"\"\n",
    "        after = \"\"\n",
    "        if f_index == frame:\n",
    "            after = ' now'\n",
    "        curr_frames.append((frame_pic, append + phrase + after))\n",
    "    X_frames.append(curr_frames)\n",
    "    \n",
    "    # prediction = findMatch(orig_imgs, proc_imgs, [phrase])[0]\n",
    "    # print(f\"Prediction for {gif_path}: Frame {prediction} vs. Actual: {frame}\")\n",
    "    # print(\"For the phrase '\" + phrase + \"'\")\n",
    "    # clear_output(wait=True)\n",
    "    # y_pred.append(prediction)\n",
    "\n",
    "# y_pred = np.array(y_pred)\n",
    "X_lengths = np.array(X_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:25]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:25]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_error = np.divide(np.abs(y_pred - photo_data_csv['Correct Frame'][0:272]), X_lengths) * 100\n",
    "plt.boxplot(y_error)\n",
    "plt.yticks(np.arange(0, 100, 10), np.char.add(np.arange(0, 100, 10).astype(\"str\"), '%'))\n",
    "plt.title(\"Error in % off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_frames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('saved_model.pth', weights_only=True, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store results\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Loop through the test data\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "        images, texts = batch\n",
    "        print(images[0])\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Calculate accuracy or other metrics\n",
    "        # Assuming the task is image-text retrieval, where the correct text for an image is its corresponding index\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        image_predictions = logits_per_image.argmax(dim=1)\n",
    "        text_predictions = logits_per_text.argmax(dim=1)\n",
    "\n",
    "        print(image_predictions)\n",
    "        # print(text_predictions)\n",
    "        print(ground_truth)\n",
    "        correct_predictions += (image_predictions == ground_truth).sum().item()\n",
    "        total_samples += len(images)\n",
    "\n",
    "# Calculate final accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose computation device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image = list_image\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(self.image[idx])\n",
    "        title = self.title[idx]\n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cycle = 1  # Adjust this between 1 and 5\n",
    "\n",
    "slice_length = len(X_frames) // 5\n",
    "\n",
    "start_index = (train_cycle - 1) * slice_length\n",
    "end_index = start_index + slice_length\n",
    "\n",
    "X_test = X_frames[start_index:end_index]\n",
    "\n",
    "X_train = X_frames[:start_index] + X_frames[end_index:]\n",
    "\n",
    "print(f\"Training data for train_cycle {train_cycle}: {len(X_train)} elements\")\n",
    "print(f\"Testing data for train_cycle {train_cycle}: {len(X_test)} elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = []\n",
    "X_test_flatten = []\n",
    "for GIF in X_train:\n",
    "    for frame in GIF:\n",
    "        X_train_flatten.append(frame)\n",
    "for GIF in X_test:\n",
    "    for frame in GIF:\n",
    "        X_test_flatten.append(frame)\n",
    "\n",
    "X_train = X_train_flatten\n",
    "X_test = X_test_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromX(X):\n",
    "    list_image = []\n",
    "    list_txt = []\n",
    "    for image, desc in X:\n",
    "        img_path = image\n",
    "        caption = desc\n",
    "        list_image.append(img_path)\n",
    "        list_txt.append(caption)\n",
    "    return list_image, list_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image_train, list_txt_train = loadFromX(X_train)\n",
    "dataset_train = image_title_dataset(list_image_train, list_txt_train)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=100, shuffle=True) #Define your own dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image_test, list_txt_test = loadFromX(X_test)\n",
    "dataset_test = image_title_dataset(list_image_test, list_txt_test)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=100, shuffle=True) #Define your own dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()\n",
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch\n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else :\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade decorator==4.4.2\n",
    "!pip install ffmpeg --upgrade\n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import sys\n",
    "\n",
    "def scale_video(input_file, output_file, target_duration):\n",
    "    # Load the video file\n",
    "    video = VideoFileClip(input_file)\n",
    "    \n",
    "    # Get the original duration of the video in seconds\n",
    "    original_duration = video.duration\n",
    "    \n",
    "    # Calculate the scaling factor\n",
    "    if original_duration > target_duration:\n",
    "        scale_factor = target_duration / original_duration\n",
    "    else:\n",
    "        scale_factor = 1  # No scaling needed if the video is shorter than the target duration\n",
    "\n",
    "    # Apply the speed-up effect to the video\n",
    "    scaled_video = video.fx(lambda clip: clip.speedx(factor=1/scale_factor))\n",
    "\n",
    "    new_audio = AudioFileClip(\"../../Desktop/tauchip.wav\")\n",
    "\n",
    "    # Set the new audio to the video clip\n",
    "    video_with_new_audio = scaled_video.set_audio(new_audio)\n",
    "    \n",
    "    # Write the result to an MP4 file\n",
    "    video_with_new_audio.write_videofile(\"output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "scale_video('../../Desktop/tauplayback.mov', '../../Desktop/tauscaled.mp4', 6*60 + 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load the video file and the WAV audio file\n",
    "video_clip = VideoFileClip(\"../../Desktop/tauscaled.mp4\")\n",
    "new_audio = AudioFileClip(\"../../Desktop/tauchip.wav\")\n",
    "\n",
    "# Set the new audio to the video clip\n",
    "video_with_new_audio = video_clip.set_audio(new_audio)\n",
    "\n",
    "# Write the result to an MP4 file\n",
    "video_with_new_audio.write_videofile(\"output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "print(\"Video and audio combined successfully into output.mp4!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
